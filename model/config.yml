# === Input Dimensions ===
input_size: 14                          # Number of features per time step (OHLCV + indicators)
sequence_length: 72                     # Number of time steps (6 hours of 5-minute candles)

# === Transformer Encoder ===
model_dimension: 128                    # Size of transformer hidden state
number_of_heads: 4                      # Attention heads (128 / 4 = 32)
number_of_layers: 3                     # Transformer encoder layers
feedforward_dimension: 256              # Feedforward sublayer dimension
dropout_rate: 0.1                       # Dropout used inside the model (not optimizer)
use_layernorm: true                     # Whether to use LayerNorm after embedding

# === Classifier Head ===
classifier_hidden_layers: [ 128, 64 ]   # MLP layers after transformer output
number_of_classes: 3                    # Output logits (buy/sell/hold)